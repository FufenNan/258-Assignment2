{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7c6577",
   "metadata": {},
   "source": [
    "## Pre-Processing Code\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter.py - filters data directly from the .json files\n",
    "import gzip\n",
    "import ast\n",
    "import tqdm\n",
    "\n",
    "def load_python_dict_gz(file_path, head=None,key=None):\n",
    "    data = []\n",
    "    count = 0\n",
    "    \n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in tqdm.tqdm(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            record = ast.literal_eval(line)\n",
    "            if key is not None and key not in record:\n",
    "                continue\n",
    "            import pdb; pdb.set_trace()\n",
    "            data.append(record)\n",
    "            count += 1\n",
    "            if head is not None and count >= head:\n",
    "                break\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_python_dict_gz(data, file_path):\n",
    "    with gzip.open(file_path, \"wt\", encoding=\"utf-8\") as f:\n",
    "        for record in data:\n",
    "            f.write(str(record))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "bundle_path = './bundle_data.json.gz'\n",
    "review_path = '../steam_reviews.json.gz'\n",
    "item_path = './australian_users_items.json.gz'\n",
    "game_path = './steam_games.json.gz'\n",
    "\n",
    "# bundles = load_python_dict_gz(bundle_path)\n",
    "# ['bundle_final_price', 'bundle_url', 'bundle_price', 'bundle_name', 'bundle_id', 'items', 'bundle_discount']\n",
    "\n",
    "reviews = load_python_dict_gz(review_path)\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "\n",
    "#88310\n",
    "users = load_python_dict_gz(item_path,key='user_id')\n",
    "\n",
    "# ['user_id', 'items_count', 'steam_id', 'user_url', 'items']\n",
    "\n",
    "#32135\n",
    "games = load_python_dict_gz(game_path,key='id')\n",
    "\n",
    "users_dict = {user['user_id']: user for user in users}\n",
    "games_dict = {game['id']: game for game in games}\n",
    "\n",
    "new_users = {}\n",
    "new_games = {}\n",
    "new_reviews = []\n",
    "count = 0\n",
    "\n",
    "# save_python_dict_gz(list(users_dict.values()), './filtered_users_items.json.gz')\n",
    "# save_python_dict_gz(list(games_dict.values()), './filtered_steam_games.json.gz')\n",
    "\n",
    "with gzip.open('./australian_user_reviews.json.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in tqdm.tqdm(f):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        record = ast.literal_eval(line)\n",
    "        if len(record['reviews']) == 0:\n",
    "            continue\n",
    "        f_review = record['reviews'][0]\n",
    "        user_id = record['user_id']\n",
    "        item_id = f_review['item_id']\n",
    "\n",
    "        if user_id in users_dict and item_id in games_dict:\n",
    "            if user_id not in new_users:\n",
    "                new_users[user_id] = users_dict[user_id]\n",
    "            if item_id not in new_games:\n",
    "                new_games[item_id] = games_dict[item_id]\n",
    "            new_reviews.append(record)\n",
    "\n",
    "print(len(new_reviews)) #23538\n",
    "print(len(list(new_users.values()))) #23250\n",
    "print(len(list(new_games.values()))) #2143\n",
    "save_python_dict_gz(new_reviews, './filtered_user_reviews.json.gz')\n",
    "save_python_dict_gz(list(new_users.values()), './filtered_users_items.json.gz')\n",
    "save_python_dict_gz(list(new_games.values()), './filtered_steam_games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1763891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process.py - converts filtered data into .pkl files\n",
    "import gzip\n",
    "import ast\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "def load_python_dict_gz(file_path, head=None):\n",
    "    data = []\n",
    "    count = 0\n",
    "    \n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in tqdm.tqdm(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            record = ast.literal_eval(line)\n",
    "\n",
    "            data.append(record)\n",
    "            count += 1\n",
    "            if head is not None and count >= head:\n",
    "                break\n",
    "\n",
    "    return data\n",
    "\n",
    "review_path = '../filtered_user_reviews.json.gz'\n",
    "item_path = '../filtered_users_items.json.gz'\n",
    "game_path = '../filtered_steam_games.json.gz'\n",
    "\n",
    "# user\n",
    "# user_id 76561197970982479\n",
    "# items_count 277\n",
    "# steam_id 76561197970982479\n",
    "# user_url http://steamcommunity.com/profiles/76561197970982479\n",
    "# items [{'item_id': '10', 'item_name': 'Counter-Strike', 'playtime_forever': 6, 'playtime_2weeks': 0}, {'item_id': '20', 'item_name': 'Team Fortress Classic', 'playtime_forever': 0, 'playtime_2weeks': 0}]\n",
    "\n",
    "\n",
    "# review\n",
    "# user_id 76561197970982479\n",
    "# user_url http://steamcommunity.com/profiles/76561197970982479\n",
    "# reviews [{'funny': '', 'posted': 'Posted November 5, 2011.', 'last_edited': '', 'item_id': '1250', 'helpful': 'No ratings yet', 'recommend': True, 'review': 'Simple yet with great replayability. In my opinion does \"zombie\" hordes and team work better than left 4 dead plus has a global leveling system. Alot of down to earth \"zombie\" splattering fun \n",
    "# for the whole family. Amazed this sort of FPS is so rare.'}, {'funny': '', 'posted': 'Posted July 15, 2011.', 'last_edited': '', 'item_id': '22200', 'helpful': 'No ratings yet', 'recommend': True, 'review': \"It's unique and worth a playthrough.\"}, {'funny': '', 'posted': 'Posted April 21, 2011.', 'last_edited': '', 'item_id': '43110', 'helpful': 'No ratings yet', 'recommend': True, 'review': 'Great atmosphere. The gunplay can be a bit chunky at times but at the end of the day this game is definitely worth it and I hope \n",
    "# they do a sequel...so buy the game so I get a sequel!'}]\n",
    "\n",
    "# game\n",
    "# publisher Kotoshiro\n",
    "# genres ['Action', 'Casual', 'Indie', 'Simulation', 'Strategy']\n",
    "# app_name Lost Summoner Kitty\n",
    "# title Lost Summoner Kitty\n",
    "# url http://store.steampowered.com/app/761140/Lost_Summoner_Kitty/\n",
    "# release_date 2018-01-04\n",
    "# tags ['Strategy', 'Action', 'Indie', 'Casual', 'Simulation']\n",
    "# discount_price 4.49\n",
    "# reviews_url http://steamcommunity.com/app/761140/reviews/?browsefilter=mostrecent&p=1\n",
    "# specs ['Single-player']\n",
    "# price 4.99\n",
    "# early_access False\n",
    "# id 761140\n",
    "# developer Kotoshiro\n",
    "\n",
    "#88310\n",
    "users = load_python_dict_gz(item_path,head=100)\n",
    "#32135\n",
    "games = load_python_dict_gz(game_path,head=100)\n",
    "reviews = load_python_dict_gz(review_path)\n",
    "\n",
    "print(users[0].keys())\n",
    "print(reviews[0].keys())\n",
    "print(games[0].keys())\n",
    "\n",
    "all_genres = set()\n",
    "all_specs = set()\n",
    "all_tags = set()\n",
    "game_dict = {}\n",
    "\n",
    "for g in games:\n",
    "    if 'id' not in g:\n",
    "        continue\n",
    "    game_dict[g['id']] = g\n",
    "    all_genres.update(g.get('genres', []))\n",
    "    all_specs.update(g.get('specs', []))\n",
    "    all_tags.update(g.get('tags', []))\n",
    "\n",
    "genres_to_idx = {g: i for i, g in enumerate(sorted(all_genres))}\n",
    "specs_to_idx = {s: i for i, s in enumerate(sorted(all_specs))}\n",
    "tags_to_idx = {t: i for i, t in enumerate(sorted(all_tags))}\n",
    "\n",
    "review_per_user = defaultdict(int)\n",
    "recommand_per_user = defaultdict(int)\n",
    "review_per_item = defaultdict(int)\n",
    "recommand_per_item = defaultdict(int)\n",
    "recommand_cnt = 0\n",
    "for r in reviews:\n",
    "    first_r = r['reviews'][0] if r['reviews'] else None\n",
    "    if first_r:\n",
    "        review_per_user[r['user_id']] += 1\n",
    "        review_per_item[first_r['item_id']] += 1\n",
    "        if first_r['recommend'] == True:\n",
    "            recommand_cnt += 1\n",
    "            recommand_per_user[r['user_id']] += 1\n",
    "            recommand_per_item[first_r['item_id']] += 1\n",
    "\n",
    "average_recommand = recommand_cnt / len(reviews)\n",
    "print(f'average_recommand: {average_recommand}')\n",
    "for k,v in recommand_per_user.items():\n",
    "    recommand_per_user[k] = v / review_per_user[k] if review_per_user[k]>0 else 0.0\n",
    "for k,v in recommand_per_item.items():\n",
    "    recommand_per_item[k] = v / review_per_item[k] if review_per_item[k]>0 else 0.0\n",
    "\n",
    "user_features = {}\n",
    "for u in users:\n",
    "    feature = {}\n",
    "    total_playtime_forever = 0\n",
    "    total_playtime_2weeks = 0\n",
    "    total_price = 0.0\n",
    "    geners_vec = np.zeros(len(genres_to_idx), dtype=np.float32)\n",
    "    specs_vec = np.zeros(len(specs_to_idx), dtype=np.float32)\n",
    "    tags_vec = np.zeros(len(tags_to_idx), dtype=np.float32)\n",
    "\n",
    "    for item in u.get('items', []):\n",
    "        total_playtime_forever += item.get('playtime_forever', 0)\n",
    "        total_playtime_2weeks += item.get('playtime_2weeks', 0)\n",
    "        price_raw = game_dict.get(item['item_id'], {}).get('price', 0.0)\n",
    "        try:\n",
    "            price = float(price_raw)\n",
    "        except (ValueError, TypeError):\n",
    "            price = 0.0\n",
    "        total_price += price\n",
    "        for g in game_dict.get(item['item_id'], {}).get('genres', []):\n",
    "            if g in genres_to_idx:\n",
    "                geners_vec[genres_to_idx[g]] += 1.0\n",
    "        for s in game_dict.get(item['item_id'], {}).get('specs', []):\n",
    "            if s in specs_to_idx:\n",
    "                specs_vec[specs_to_idx[s]] += 1.0\n",
    "        for t in game_dict.get(item['item_id'], {}).get('tags', []):\n",
    "            if t in tags_to_idx:\n",
    "                tags_vec[tags_to_idx[t]] += 1.0\n",
    "    tmp = u.get('items', [])\n",
    "    feature['items_count'] = u.get('items_count', 0)\n",
    "    feature['playtime_forever'] = total_playtime_forever/feature['items_count'] if feature['items_count']>0 else 0\n",
    "    feature['playtime_2weeks'] = total_playtime_2weeks/feature['items_count'] if feature['items_count']>0 else 0\n",
    "    feature['average_price'] = total_price/feature['items_count'] if feature['items_count']>0 else 0\n",
    "    feature['genres_vec'] = geners_vec\n",
    "    feature['specs_vec'] = specs_vec\n",
    "    feature['tags_vec'] = tags_vec\n",
    "    # No recommendation ratio because the data sparsity\n",
    "    user_features[u['user_id']] = feature\n",
    "\n",
    "print(len(user_features))\n",
    "with open('user_features.pkl', 'wb') as f:\n",
    "    pickle.dump(user_features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "game_features = {}\n",
    "for game in games:\n",
    "    feature = {}\n",
    "    genres_vec = np.zeros(len(genres_to_idx), dtype=np.float32)\n",
    "    specs_vec = np.zeros(len(specs_to_idx), dtype=np.float32)\n",
    "    tags_vec = np.zeros(len(tags_to_idx), dtype=np.float32)\n",
    "\n",
    "    for g in game.get('genres', []):\n",
    "        if g in genres_to_idx:\n",
    "            genres_vec[genres_to_idx[g]] = 1.0\n",
    "    for s in game.get('specs', []):\n",
    "        if s in specs_to_idx:\n",
    "            specs_vec[specs_to_idx[s]] = 1.0\n",
    "    for t in game.get('tags', []):\n",
    "        if t in tags_to_idx:\n",
    "            tags_vec[tags_to_idx[t]] = 1.0\n",
    "    price_raw = game_dict.get(game['id'], {}).get('price', 0.0)\n",
    "    try:\n",
    "        price = float(price_raw)\n",
    "    except (ValueError, TypeError):\n",
    "        price = 0.0\n",
    "    feature['price'] = price\n",
    "    feature['genres_vec'] = genres_vec\n",
    "    feature['specs_vec'] = specs_vec\n",
    "    feature['tags_vec'] = tags_vec\n",
    "    feature['review_count'] = review_per_item.get(game['id'], 0)\n",
    "    feature['recommend_ratio'] = recommand_per_item.get(game['id'], average_recommand)\n",
    "    game_features[game['id']] = feature\n",
    "\n",
    "print(len(game_features))\n",
    "with open('game_features.pkl', 'wb') as f:\n",
    "    pickle.dump(game_features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "play_time_forever = {}\n",
    "play_time_2weeks = {}\n",
    "for u in users:\n",
    "    for item in u.get('items', []):\n",
    "        play_time_forever[(u['user_id'], item['item_id'])] = item.get('playtime_forever', 0)\n",
    "        play_time_2weeks[(u['user_id'], item['item_id'])] = item.get('playtime_2weeks', 0)\n",
    "\n",
    "training_data = []\n",
    "for r in reviews:\n",
    "    x = {}\n",
    "    first_r = r['reviews'][0]\n",
    "    user_id = r['user_id']\n",
    "    item_id = first_r['item_id']\n",
    "    user_feature = user_features[user_id]\n",
    "    game_feature = game_features[item_id]\n",
    "    cross_feature = {}\n",
    "    cross_feature['playtime_forever'] = play_time_forever.get((user_id, item_id), 0)\n",
    "    cross_feature['playtime_2weeks'] = play_time_2weeks.get((user_id, item_id), 0)\n",
    "\n",
    "    x['user_feature'] = user_feature\n",
    "    x['game_feature'] = game_feature\n",
    "    x['cross_feature'] = cross_feature\n",
    "    x['label'] = 1 if first_r['recommend'] == True else 0\n",
    "    training_data.append(x)\n",
    "\n",
    "with open('training_data.pkl', 'wb') as f:\n",
    "    pickle.dump(training_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('training_data.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "for i, x in enumerate(training_data[:2]):\n",
    "    print(f\"Sample {i}:\")\n",
    "    for k, v in x['user_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            print(f\"User feature - {k}: array of shape {v.shape}\")\n",
    "        else:\n",
    "            print(f\"User feature - {k}: {v}\")\n",
    "    for k, v in x['game_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            print(f\"Game feature - {k}: array of shape {v.shape}\")\n",
    "        else:\n",
    "            print(f\"Game feature - {k}: {v}\")\n",
    "    for k, v in x['cross_feature'].items():\n",
    "        print(f\"Cross feature - {k}: {v}\")\n",
    "    print(f\"Label: {x['label']}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view.py - converts .pkl files into tensors for training\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "with open('training_data.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "for i, x in enumerate(training_data[:2]):\n",
    "    print(f\"Sample {i}:\")\n",
    "    for k, v in x['user_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            print(f\"User feature - {k}: array of shape {v.shape}\")\n",
    "        else:\n",
    "            print(f\"User feature - {k}: {v}\")\n",
    "    for k, v in x['game_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            print(f\"Game feature - {k}: array of shape {v.shape}\")\n",
    "        else:\n",
    "            print(f\"Game feature - {k}: {v}\")\n",
    "    for k, v in x['cross_feature'].items():\n",
    "        print(f\"Cross feature - {k}: {v}\")\n",
    "    print(f\"Label: {x['label']}\")\n",
    "    print(\"-----\")\n",
    "\n",
    "X_user = []\n",
    "X_game = []\n",
    "X_cross = []\n",
    "y = []\n",
    "\n",
    "for x in training_data:\n",
    "    # User features\n",
    "    user_feats = []\n",
    "    for k, v in x['user_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            user_feats.extend(v.tolist())\n",
    "        else:\n",
    "            user_feats.append(v)\n",
    "    X_user.append(user_feats)\n",
    "\n",
    "    # Game features\n",
    "    game_feats = []\n",
    "    for k, v in x['game_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            game_feats.extend(v.tolist())\n",
    "        else:\n",
    "            game_feats.append(v)\n",
    "    X_game.append(game_feats)\n",
    "\n",
    "    # Cross features\n",
    "    cross_feats = []\n",
    "    for k, v in x['cross_feature'].items():\n",
    "        cross_feats.append(v)\n",
    "    X_cross.append(cross_feats)\n",
    "\n",
    "    # Label\n",
    "    y.append(x['label'])\n",
    "\n",
    "X_user = np.array(X_user, dtype=np.float32)\n",
    "X_game = np.array(X_game, dtype=np.float32)\n",
    "X_cross = np.array(X_cross, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "\n",
    "X = np.concatenate([X_user, X_game, X_cross], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "print('length of training data:', X_train.shape[0])\n",
    "print('length of test data:', X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a109f",
   "metadata": {},
   "source": [
    "## Model Architectures\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d6b7b",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Decision Trees (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672feb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "TRAINING_DATA_PATH = \".//data//training_data.pkl\"\n",
    "\n",
    "# code taken from view.py\n",
    "def load_tensors(path, seed=42, test_size=0.2):\n",
    "    # load pkl file\n",
    "    with open(path, 'rb') as f:\n",
    "        training_data = pickle.load(f)\n",
    "        \n",
    "    X_user = []\n",
    "    X_game = []\n",
    "    X_cross = []\n",
    "    y = []\n",
    "\n",
    "    for x in training_data:\n",
    "        # User features\n",
    "        user_feats = []\n",
    "        for k, v in x['user_feature'].items():\n",
    "            if isinstance(v, np.ndarray):\n",
    "                user_feats.extend(v.tolist())\n",
    "            else:\n",
    "                user_feats.append(v)\n",
    "        X_user.append(user_feats)\n",
    "\n",
    "        # Game features\n",
    "        game_feats = []\n",
    "        for k, v in x['game_feature'].items():\n",
    "            if isinstance(v, np.ndarray):\n",
    "                game_feats.extend(v.tolist())\n",
    "            else:\n",
    "                game_feats.append(v)\n",
    "        X_game.append(game_feats)\n",
    "\n",
    "        # Cross features\n",
    "        cross_feats = []\n",
    "        for k, v in x['cross_feature'].items():\n",
    "            cross_feats.append(v)\n",
    "        X_cross.append(cross_feats)\n",
    "\n",
    "        # Label\n",
    "        y.append(x['label'])\n",
    "\n",
    "    X_user = np.array(X_user, dtype=np.float32)\n",
    "    X_game = np.array(X_game, dtype=np.float32)\n",
    "    X_cross = np.array(X_cross, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "\n",
    "    X = np.concatenate([X_user, X_game, X_cross], axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, shuffle=True, stratify=y\n",
    "    )\n",
    "    \n",
    "    # return training and testing data\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# init tensors\n",
    "x_train, x_test, y_train, y_test = load_tensors(TRAINING_DATA_PATH)\n",
    "\n",
    "# optuna hyperparam tuning\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "    # hyper param search space\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 5000),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 0.001, 0.75, log=True),\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier( \n",
    "        **params,\n",
    "        device='cuda',\n",
    "        use_label_encoder=False,\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(x_test)[:, 1]\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    # return value optuna will try to maximize\n",
    "    return macro_f1\n",
    "    \n",
    "\n",
    "# --- script entry point ---\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"xgb_recommender\",\n",
    ")   \n",
    "\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(\"Best AUC:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n",
    "best_params = study.best_params.copy()\n",
    "best_params.update({\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"tree_method\": \"hist\",\n",
    "})\n",
    "\n",
    "best_model = xgb.XGBClassifier(\n",
    "    **best_params,\n",
    "    device='cuda',\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "\n",
    "best_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# eval on test set\n",
    "y_test_proba = best_model.predict_proba(x_test)[:, 1]\n",
    "y_test_pred = (y_test_proba >= 0.5).astype(int)\n",
    "\n",
    "test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "print(f\"Test F1:  {test_f1:.4f}\")\n",
    "print(f\"Test Acc: {test_acc:.4f}\")\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_test_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cace05f",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load training data\n",
    "with open('data/training_data.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "# print(f\"Total samples: {len(training_data)}\")\n",
    "\n",
    "# Extract features and labels\n",
    "X_user = []\n",
    "X_game = []\n",
    "X_cross = []\n",
    "y = []\n",
    "\n",
    "for x in training_data:\n",
    "    # User features\n",
    "    user_feats = []\n",
    "    for k, v in x['user_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            user_feats.extend(v.tolist())\n",
    "        else:\n",
    "            user_feats.append(v)\n",
    "    X_user.append(user_feats)\n",
    "\n",
    "    # Game features\n",
    "    game_feats = []\n",
    "    for k, v in x['game_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            game_feats.extend(v.tolist())\n",
    "        else:\n",
    "            game_feats.append(v)\n",
    "    X_game.append(game_feats)\n",
    "\n",
    "    # Cross features\n",
    "    cross_feats = []\n",
    "    for k, v in x['cross_feature'].items():\n",
    "        cross_feats.append(v)\n",
    "    X_cross.append(cross_feats)\n",
    "\n",
    "    # Label\n",
    "    y.append(x['label'])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_user = np.array(X_user, dtype=np.float32)\n",
    "X_game = np.array(X_game, dtype=np.float32)\n",
    "X_cross = np.array(X_cross, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "\n",
    "# Concatenate all features\n",
    "X = np.concatenate([X_user, X_game, X_cross], axis=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# print(f\"Training set size: {X_train.shape[0]}\")\n",
    "# print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Create and train the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy (%): {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Recommend', 'Recommend']))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives: {cm[0,0]}\")\n",
    "print(f\"False Positives: {cm[0,1]}\")\n",
    "print(f\"False Negatives: {cm[1,0]}\")\n",
    "print(f\"True Positives: {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ceced3",
   "metadata": {},
   "source": [
    "#### Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79831929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the preprocessed data\n",
    "with open('../data/training_data.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "print(training_data[0])\n",
    "print(len(training_data))\n",
    "\n",
    "# Prepare features and labels\n",
    "X_user = []\n",
    "X_game = []\n",
    "X_cross = []\n",
    "y = []\n",
    "\n",
    "for x in training_data:\n",
    "    # User features\n",
    "    user_feats = []\n",
    "    for k, v in x['user_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            user_feats.extend(v.tolist())\n",
    "        else:\n",
    "            user_feats.append(v)\n",
    "    X_user.append(user_feats)\n",
    "    \n",
    "    # Game features\n",
    "    game_feats = []\n",
    "    for k, v in x['game_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            game_feats.extend(v.tolist())\n",
    "        else:\n",
    "            game_feats.append(v)\n",
    "    X_game.append(game_feats)\n",
    "    \n",
    "    # Cross features\n",
    "    cross_feats = []\n",
    "    for k, v in x['cross_feature'].items():\n",
    "        cross_feats.append(v)\n",
    "    X_cross.append(cross_feats)\n",
    "    \n",
    "    # Label\n",
    "    y.append(x['label'])\n",
    "\n",
    "# Combine all features\n",
    "X = np.concatenate([\n",
    "    np.array(X_user, dtype=np.float32),\n",
    "    np.array(X_game, dtype=np.float32),\n",
    "    np.array(X_cross, dtype=np.float32)\n",
    "], axis=1)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Feature dimension: {X_train.shape[1]}\")\n",
    "print(f\"Positive rate in training: {y_train.mean():.3f}\")\n",
    "print(f\"Positive rate in test: {y_test.mean():.3f}\")\n",
    "print()\n",
    "\n",
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression model...\")\n",
    "lr_model = LogisticRegression(max_iter=10000, random_state=42, class_weight='balanced', solver='liblinear', C=1.0)\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"Training complete!\")\n",
    "print()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = lr_model.predict(X_train)\n",
    "y_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report: Training Set\")\n",
    "print(classification_report(y_train, y_pred_train, target_names=['Not Recommend', 'Recommend']))\n",
    "\n",
    "print(\"Evaluation Metrics: Training Set\")\n",
    "train_precision, train_recall, train_f1, train_support = precision_recall_fscore_support(\n",
    "    y_train, y_pred_train, average=None, labels=[0, 1]\n",
    ")\n",
    "\n",
    "print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\n",
    "print(f\"{'Not Recommend':<15} {train_precision[0]:<12.4f} {train_recall[0]:<12.4f} {train_f1[0]:<12.4f} {train_support[0]:<12}\")\n",
    "print(f\"{'Recommend':<15} {train_precision[1]:<12.4f} {train_recall[1]:<12.4f} {train_f1[1]:<12.4f} {train_support[1]:<12}\")\n",
    "print(f\"{'Accuracy':<15} {'':<12} {'':<12} {accuracy_score(y_train, y_pred_train):<12.4f} {train_support.sum():<12}\")\n",
    "print(f\"{'Macro Avg':<15} {precision_score(y_train, y_pred_train, average='macro'):<12.4f} {recall_score(y_train, y_pred_train, average='macro'):<12.4f} {f1_score(y_train, y_pred_train, average='macro'):<12.4f} {train_support.sum():<12}\")\n",
    "print(f\"{'Weighted Avg':<15} {precision_score(y_train, y_pred_train, average='weighted'):<12.4f} {recall_score(y_train, y_pred_train, average='weighted'):<12.4f} {f1_score(y_train, y_pred_train, average='weighted'):<12.4f} {train_support.sum():<12}\")\n",
    "print()\n",
    "\n",
    "print(\"Confusion Matrix: Training Set\")\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "print(cm_train)\n",
    "print()\n",
    "print(f\"True Negatives (TN): {cm_train[0,0]}\")\n",
    "print(f\"False Positives (FP): {cm_train[0,1]}\")\n",
    "print(f\"False Negatives (FN): {cm_train[1,0]}\")\n",
    "print(f\"True Positives (TP): {cm_train[1,1]}\")\n",
    "print()\n",
    "\n",
    "print(\"Classification Report: Test Set\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Not Recommend', 'Recommend']))\n",
    "\n",
    "print(\"Evaluation Metrics: Test Set\")\n",
    "test_precision, test_recall, test_f1, test_support = precision_recall_fscore_support(\n",
    "    y_test, y_pred_test, average=None, labels=[0, 1]\n",
    ")\n",
    "\n",
    "print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\n",
    "print(f\"{'Not Recommend':<15} {test_precision[0]:<12.4f} {test_recall[0]:<12.4f} {test_f1[0]:<12.4f} {test_support[0]:<12}\")\n",
    "print(f\"{'Recommend':<15} {test_precision[1]:<12.4f} {test_recall[1]:<12.4f} {test_f1[1]:<12.4f} {test_support[1]:<12}\")\n",
    "print(f\"{'Accuracy':<15} {'':<12} {'':<12} {accuracy_score(y_test, y_pred_test):<12.4f} {test_support.sum():<12}\")\n",
    "print(f\"{'Macro Avg':<15} {precision_score(y_test, y_pred_test, average='macro'):<12.4f} {recall_score(y_test, y_pred_test, average='macro'):<12.4f} {f1_score(y_test, y_pred_test, average='macro'):<12.4f} {test_support.sum():<12}\")\n",
    "print(f\"{'Weighted Avg':<15} {precision_score(y_test, y_pred_test, average='weighted'):<12.4f} {recall_score(y_test, y_pred_test, average='weighted'):<12.4f} {f1_score(y_test, y_pred_test, average='weighted'):<12.4f} {test_support.sum():<12}\")\n",
    "print()\n",
    "\n",
    "print(\"Confusion Matrix: Test Set\")\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print(cm_test)\n",
    "print()\n",
    "print(f\"True Negatives (TN): {cm_test[0,0]}\")\n",
    "print(f\"False Positives (FP): {cm_test[0,1]}\")\n",
    "print(f\"False Negatives (FN): {cm_test[1,0]}\")\n",
    "print(f\"True Positives (TP): {cm_test[1,1]}\")\n",
    "print()\n",
    "\n",
    "print(\"Comparison of Training and Test Set Metrics\")\n",
    "print(f\"{'Metric':<25} {'Training':<20} {'Test':<20}\")\n",
    "print(f\"{'Accuracy':<25} {accuracy_score(y_train, y_pred_train):<20.4f} {accuracy_score(y_test, y_pred_test):<20.4f}\")\n",
    "print(f\"{'Precision (Macro)':<25} {precision_score(y_train, y_pred_train, average='macro'):<20.4f} {precision_score(y_test, y_pred_test, average='macro'):<20.4f}\")\n",
    "print(f\"{'Recall (Macro)':<25} {recall_score(y_train, y_pred_train, average='macro'):<20.4f} {recall_score(y_test, y_pred_test, average='macro'):<20.4f}\")\n",
    "print(f\"{'F1-Score (Macro)':<25} {f1_score(y_train, y_pred_train, average='macro'):<20.4f} {f1_score(y_test, y_pred_test, average='macro'):<20.4f}\")\n",
    "print(f\"{'Precision (Weighted)':<25} {precision_score(y_train, y_pred_train, average='weighted'):<20.4f} {precision_score(y_test, y_pred_test, average='weighted'):<20.4f}\")\n",
    "print(f\"{'Recall (Weighted)':<25} {recall_score(y_train, y_pred_train, average='weighted'):<20.4f} {recall_score(y_test, y_pred_test, average='weighted'):<20.4f}\")\n",
    "print(f\"{'F1-Score (Weighted)':<25} {f1_score(y_train, y_pred_train, average='weighted'):<20.4f} {f1_score(y_test, y_pred_test, average='weighted'):<20.4f}\")\n",
    "print()\n",
    "\n",
    "# Plot both confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training confusion matrix\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Not Recommend', 'Recommend'],\n",
    "            yticklabels=['Not Recommend', 'Recommend'])\n",
    "axes[0].set_title('Confusion Matrix - Training Set')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Test confusion matrix\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=['Not Recommend', 'Recommend'],\n",
    "            yticklabels=['Not Recommend', 'Recommend'])\n",
    "axes[1].set_title('Confusion Matrix - Test Set')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "feature_names = []\n",
    "# User features\n",
    "for k in training_data[0]['user_feature'].keys():\n",
    "    if isinstance(training_data[0]['user_feature'][k], np.ndarray):\n",
    "        feature_names.extend([f'user_{k}_{i}' for i in range(len(training_data[0]['user_feature'][k]))])\n",
    "    else:\n",
    "        feature_names.append(f'user_{k}')\n",
    "# Game features\n",
    "for k in training_data[0]['game_feature'].keys():\n",
    "    if isinstance(training_data[0]['game_feature'][k], np.ndarray):\n",
    "        feature_names.extend([f'game_{k}_{i}' for i in range(len(training_data[0]['game_feature'][k]))])\n",
    "    else:\n",
    "        feature_names.append(f'game_{k}')\n",
    "# Cross features\n",
    "for k in training_data[0]['cross_feature'].keys():\n",
    "    feature_names.append(f'cross_{k}')\n",
    "\n",
    "coefficients = lr_model.coef_[0]\n",
    "feature_importance = list(zip(feature_names, coefficients))\n",
    "feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(f\"{'Rank':<6} {'Feature':<45} {'Coefficient':<15}\")\n",
    "for i, (name, coef) in enumerate(feature_importance[:20], 1):\n",
    "    print(f\"{i:<6} {name:<45} {coef:>15.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f6809",
   "metadata": {},
   "source": [
    "#### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "with open('./data/training_data.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "# Prepare features and labels\n",
    "X_user = []\n",
    "X_game = []\n",
    "X_cross = []\n",
    "y = []\n",
    "\n",
    "for x in training_data:\n",
    "    # User features\n",
    "    user_feats = []\n",
    "    for k, v in x['user_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            user_feats.extend(v.tolist())\n",
    "        else:\n",
    "            user_feats.append(v)\n",
    "    X_user.append(user_feats)\n",
    "    \n",
    "    # Game features\n",
    "    game_feats = []\n",
    "    for k, v in x['game_feature'].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            game_feats.extend(v.tolist())\n",
    "        else:\n",
    "            game_feats.append(v)\n",
    "    X_game.append(game_feats)\n",
    "    \n",
    "    # Cross features\n",
    "    cross_feats = []\n",
    "    for k, v in x['cross_feature'].items():\n",
    "        cross_feats.append(v)\n",
    "    X_cross.append(cross_feats)\n",
    "    \n",
    "    # Label\n",
    "    y.append(x['label'])\n",
    "\n",
    "# Combine all features\n",
    "X = np.concatenate([\n",
    "    np.array(X_user, dtype=np.float32),\n",
    "    np.array(X_game, dtype=np.float32),\n",
    "    np.array(X_cross, dtype=np.float32)\n",
    "], axis=1)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Feature dimension: {X_train.shape[1]}\")\n",
    "print(f\"Positive rate in training: {y_train.mean():.3f}\")\n",
    "print(f\"Positive rate in test: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Train Linear SVM\n",
    "# ----------------------\n",
    "print(\"Training Linear SVM model...\")\n",
    "\n",
    "svm_model = LinearSVC(\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=20000,\n",
    "    C=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print()\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = svm_model.predict(X_train)\n",
    "y_pred_test = svm_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report: Training Set\")\n",
    "print(classification_report(y_train, y_pred_train, target_names=['Not Recommend', 'Recommend']))\n",
    "\n",
    "print(\"Evaluation Metrics: Training Set\")\n",
    "train_precision, train_recall, train_f1, train_support = precision_recall_fscore_support(\n",
    "    y_train, y_pred_train, average=None, labels=[0, 1]\n",
    ")\n",
    "\n",
    "print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\n",
    "print(f\"{'Not Recommend':<15} {train_precision[0]:<12.4f} {train_recall[0]:<12.4f} {train_f1[0]:<12.4f} {train_support[0]:<12}\")\n",
    "print(f\"{'Recommend':<15} {train_precision[1]:<12.4f} {train_recall[1]:<12.4f} {train_f1[1]:<12.4f} {train_support[1]:<12}\")\n",
    "print(f\"{'Accuracy':<15} {'':<12} {'':<12} {accuracy_score(y_train, y_pred_train):<12.4f} {train_support.sum():<12}\")\n",
    "print(f\"{'Macro Avg':<15} {precision_score(y_train, y_pred_train, average='macro'):<12.4f} {recall_score(y_train, y_pred_train, average='macro'):<12.4f} {f1_score(y_train, y_pred_train, average='macro'):<12.4f} {train_support.sum():<12}\")\n",
    "print(f\"{'Weighted Avg':<15} {precision_score(y_train, y_pred_train, average='weighted'):<12.4f} {recall_score(y_train, y_pred_train, average='weighted'):<12.4f} {f1_score(y_train, y_pred_train, average='weighted'):<12.4f} {train_support.sum():<12}\")\n",
    "print()\n",
    "\n",
    "print(\"Confusion Matrix: Training Set\")\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "print(cm_train)\n",
    "print()\n",
    "print(f\"True Negatives (TN): {cm_train[0,0]}\")\n",
    "print(f\"False Positives (FP): {cm_train[0,1]}\")\n",
    "print(f\"False Negatives (FN): {cm_train[1,0]}\")\n",
    "print(f\"True Positives (TP): {cm_train[1,1]}\")\n",
    "print()\n",
    "\n",
    "print(\"Classification Report: Test Set\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Not Recommend', 'Recommend']))\n",
    "\n",
    "print(\"Evaluation Metrics: Test Set\")\n",
    "test_precision, test_recall, test_f1, test_support = precision_recall_fscore_support(\n",
    "    y_test, y_pred_test, average=None, labels=[0, 1]\n",
    ")\n",
    "\n",
    "print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\n",
    "print(f\"{'Not Recommend':<15} {test_precision[0]:<12.4f} {test_recall[0]:<12.4f} {test_f1[0]:<12.4f} {test_support[0]:<12}\")\n",
    "print(f\"{'Recommend':<15} {test_precision[1]:<12.4f} {test_recall[1]:<12.4f} {test_f1[1]:<12.4f} {test_support[1]:<12}\")\n",
    "print(f\"{'Accuracy':<15} {'':<12} {'':<12} {accuracy_score(y_test, y_pred_test):<12.4f} {test_support.sum():<12}\")\n",
    "print(f\"{'Macro Avg':<15} {precision_score(y_test, y_pred_test, average='macro'):<12.4f} {recall_score(y_test, y_pred_test, average='macro'):<12.4f} {f1_score(y_test, y_pred_test, average='macro'):<12.4f} {test_support.sum():<12}\")\n",
    "print(f\"{'Weighted Avg':<15} {precision_score(y_test, y_pred_test, average='weighted'):<12.4f} {recall_score(y_test, y_pred_test, average='weighted'):<12.4f} {f1_score(y_test, y_pred_test, average='weighted'):<12.4f} {test_support.sum():<12}\")\n",
    "print()\n",
    "\n",
    "print(\"Confusion Matrix: Test Set\")\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print(cm_test)\n",
    "print()\n",
    "print(f\"True Negatives (TN): {cm_test[0,0]}\")\n",
    "print(f\"False Positives (FP): {cm_test[0,1]}\")\n",
    "print(f\"False Negatives (FN): {cm_test[1,0]}\")\n",
    "print(f\"True Positives (TP): {cm_test[1,1]}\")\n",
    "print()\n",
    "\n",
    "print(\"Comparison of Training and Test Set Metrics\")\n",
    "print(f\"{'Metric':<25} {'Training':<20} {'Test':<20}\")\n",
    "print(f\"{'Accuracy':<25} {accuracy_score(y_train, y_pred_train):<20.4f} {accuracy_score(y_test, y_pred_test):<20.4f}\")\n",
    "print(f\"{'Precision (Macro)':<25} {precision_score(y_train, y_pred_train, average='macro'):<20.4f} {precision_score(y_test, y_pred_test, average='macro'):<20.4f}\")\n",
    "print(f\"{'Recall (Macro)':<25} {recall_score(y_train, y_pred_train, average='macro'):<20.4f} {recall_score(y_test, y_pred_test, average='macro'):<20.4f}\")\n",
    "print(f\"{'F1-Score (Macro)':<25} {f1_score(y_train, y_pred_train, average='macro'):<20.4f} {f1_score(y_test, y_pred_test, average='macro'):<20.4f}\")\n",
    "print(f\"{'Precision (Weighted)':<25} {precision_score(y_train, y_pred_train, average='weighted'):<20.4f} {precision_score(y_test, y_pred_test, average='weighted'):<20.4f}\")\n",
    "print(f\"{'Recall (Weighted)':<25}    {recall_score(y_train, y_pred_train, average='weighted'):<20.4f} {recall_score(y_test, y_pred_test, average='weighted'):<20.4f}\")\n",
    "print(f\"{'F1-Score (Weighted)':<25}  {f1_score(y_train, y_pred_train, average='weighted'):<20.4f} {f1_score(y_test, y_pred_test, average='weighted'):<20.4f}\")\n",
    "\n",
    "\n",
    "# Plot both confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Not Recommend', 'Recommend'],\n",
    "            yticklabels=['Not Recommend', 'Recommend'])\n",
    "axes[0].set_title('Confusion Matrix - Training Set')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=['Not Recommend', 'Recommend'],\n",
    "            yticklabels=['Not Recommend', 'Recommend'])\n",
    "axes[1].set_title('Confusion Matrix - Test Set')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ----------------------\n",
    "# Feature Importance (Linear SVM supports coef_)\n",
    "# ----------------------\n",
    "feature_names = []\n",
    "# User features\n",
    "for k in training_data[0]['user_feature'].keys():\n",
    "    if isinstance(training_data[0]['user_feature'][k], np.ndarray):\n",
    "        feature_names.extend([f'user_{k}_{i}' for i in range(len(training_data[0]['user_feature'][k]))])\n",
    "    else:\n",
    "        feature_names.append(f'user_{k}')\n",
    "# Game features\n",
    "for k in training_data[0]['game_feature'].keys():\n",
    "    if isinstance(training_data[0]['game_feature'][k], np.ndarray):\n",
    "        feature_names.extend([f'game_{k}_{i}' for i in range(len(training_data[0]['game_feature'][k]))])\n",
    "    else:\n",
    "        feature_names.append(f'game_{k}')\n",
    "# Cross features\n",
    "for k in training_data[0]['cross_feature'].keys():\n",
    "    feature_names.append(f'cross_{k}')\n",
    "\n",
    "coefficients = svm_model.coef_[0]\n",
    "feature_importance = list(zip(feature_names, coefficients))\n",
    "feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(f\"{'Rank':<6} {'Feature':<45} {'Coefficient':<15}\")\n",
    "for i, (name, coef) in enumerate(feature_importance[:20], 1):\n",
    "    print(f\"{i:<6} {name:<45} {coef:>15.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
